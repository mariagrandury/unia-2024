{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariagrandury/unia-2024/blob/main/creacion_de_datasets_sinteticos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEkqXf1JaAYt"
      },
      "source": [
        "# Datasets de alineamiento de LLMs en español\n",
        "\n",
        "## ¿Hay datasets para alinear modelos en español?\n",
        "\n",
        "Vamos a echar un vistazo a hf.co\n",
        "\n",
        "## Configuración del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0vAu88eaAYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e626cac-9509-4872-9b61-7ca9671e6b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/527.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m522.2/527.3 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub -q\n",
        "!pip install datasets pandas -q\n",
        "!pip install \"argilla==2.0.0\" -q\n",
        "!pip install trl -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elQ4jPHYaAYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9859fe-96a9-4498-d20f-42b3c9396dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgGgcRCxaAYw"
      },
      "source": [
        "## Creación de datasets sintéticos con Argilla\n",
        "\n",
        "- Espacio de anotación: https://huggingface.co/spaces/mariagrandury/alineamiento-de-modelos\n",
        "- Crea tu propio espacio de anotación: http://huggingface.co/new-space?template=argilla/argilla-template-space&name=my-argilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-3rWU_9aAYx"
      },
      "outputs": [],
      "source": [
        "import argilla as rg\n",
        "\n",
        "client = rg.Argilla(\n",
        "    api_url=\"https://mariagrandury-alineamiento-de-modelos.hf.space\", # URL del Space, termina en *.hf.space\n",
        "    api_key=\"<api_key>\" # la puedes encontrar en My Settings de tu Argilla Space\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLbiQTNUaAYx"
      },
      "source": [
        "## Crear un dataset de preguntas y respuestas (SFT)\n",
        "\n",
        "Formato esperado:\n",
        "- `prompt`\n",
        "- `completion`\n",
        "\n",
        "### 1. Configurar el dataset de Argilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo9qtUXgaAYx",
        "outputId": "bbe07e28-3741-4332-8007-df3440504e39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset(id=UUID('e06cea79-cb6c-4997-96a2-2df088e0f482') inserted_at=datetime.datetime(2024, 8, 20, 14, 8, 2, 555448) updated_at=datetime.datetime(2024, 8, 20, 14, 8, 4, 61312) name='prompts-dataset' status='ready' guidelines='Please, read the prompt carefully and write a response' allow_extra_metadata=False distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('2f993734-0774-4016-a636-7c11251b2c64') last_activity_at=datetime.datetime(2024, 8, 20, 14, 8, 4, 61312))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import argilla as rg\n",
        "\n",
        "settings = rg.Settings(\n",
        "    guidelines=\"Please, read the prompt carefully and write a response\",\n",
        "    fields = [\n",
        "        rg.TextField(name=\"prompt\", required=True),\n",
        "    ],\n",
        "    questions = [\n",
        "    rg.TextQuestion(\n",
        "        name=\"completion\",\n",
        "        title=\"Please write an accurate, helpful, and harmless response to the prompt\",\n",
        "        required=True,\n",
        "    )\n",
        "    ],\n",
        ")\n",
        "\n",
        "dataset = rg.Dataset(\n",
        "    name=\"prompts-dataset\",\n",
        "    workspace=\"argilla\", # nombre del workspace creado por defecto\n",
        "    client=client,\n",
        "    settings=settings,\n",
        ")\n",
        "dataset.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxqHrYEvaAYx"
      },
      "source": [
        "### 2. Añadir prompts\n",
        "\n",
        "- Dataset: https://huggingface.co/datasets/somosnlp/prompt-translation-for-es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKmPEuCNaAYx",
        "outputId": "c8fe5584-71d9-409b-8cd6-1125ff976ef0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 10/10 [00:00<00:00, 167.91 examples/s]\n",
            "/opt/homebrew/lib/python3.11/site-packages/argilla/records/_mapping/_mapper.py:89: UserWarning: Keys ['input', 'target', 'target-suggestion', 'target-suggestion-metadata', 'external_id', 'metadata', 'generation_model', 'generation_prompt', 'raw_generation_responses', 'generations'] in data are not present in the mapping and will be ignored.\n",
            "  warnings.warn(f\"Keys {unknown_keys} in data are not present in the mapping and will be ignored.\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatasetRecords: The provided batch size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span> was normalized. Using value <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "DatasetRecords: The provided batch size \u001b[1;36m256\u001b[0m was normalized. Using value \u001b[1;36m10\u001b[0m.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sending records...: 100%|██████████| 1/1 [00:09<00:00,  9.27s/batch]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetRecords(Dataset(id=UUID('e06cea79-cb6c-4997-96a2-2df088e0f482') inserted_at=datetime.datetime(2024, 8, 20, 14, 8, 2, 555448) updated_at=datetime.datetime(2024, 8, 20, 14, 8, 4, 61312) name='prompts-dataset' status='ready' guidelines='Please, read the prompt carefully and write a response' allow_extra_metadata=False distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('2f993734-0774-4016-a636-7c11251b2c64') last_activity_at=datetime.datetime(2024, 8, 20, 14, 8, 4, 61312)))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "prompts = load_dataset('somosnlp/prompt-translation-for-es', split=\"train[:10]\")\n",
        "prompts.select_columns([\"generations\"])\n",
        "\n",
        "def get_generation(example):\n",
        "    example[\"generations\"] = example[\"generations\"][0]\n",
        "    return example\n",
        "\n",
        "new_ds = prompts.map(get_generation)\n",
        "\n",
        "dataset.records.log(records=new_ds, mapping={\"generations\": \"prompt\"}) # mapear columna del dataset con campo del espacio de anotación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCmqMiXLaAYz"
      },
      "source": [
        "## Crear un dataset de preferencia: KTO\n",
        "\n",
        "Empezamos con el algoritmo sencillito, necesitamos:\n",
        "- `prompt`: pregunta\n",
        "- `response`: respuesta\n",
        "- `label`: respuesta buena o mala"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear un dataset en Argilla"
      ],
      "metadata": {
        "id": "YHt9F5SY22Yr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP5S2F0naAYz",
        "outputId": "a31b8b1f-173a-458a-f0d4-65f1d5cb74ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset(id=UUID('5351f515-c6d8-470e-ad49-dda69ef19186') inserted_at=datetime.datetime(2024, 8, 20, 22, 53, 15, 361405) updated_at=datetime.datetime(2024, 8, 20, 22, 53, 16, 461340) name='kpo-dataset-2' status='ready' guidelines='Lee la pregunta y determina si la respuesta está bien o mal teniendo en cuenta...' allow_extra_metadata=False distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('2f993734-0774-4016-a636-7c11251b2c64') last_activity_at=datetime.datetime(2024, 8, 20, 22, 53, 16, 461340))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import argilla as rg\n",
        "\n",
        "kto_settings = rg.Settings(\n",
        "    guidelines=\"Lee la pregunta y determina si la respuesta está bien o mal teniendo en cuenta...\",\n",
        "    fields = [\n",
        "        rg.TextField(name=\"prompt\", required=True),\n",
        "        rg.TextField(name=\"response\", required=True),\n",
        "    ],\n",
        "    questions = [\n",
        "        rg.LabelQuestion(\n",
        "            name=\"label\",\n",
        "            title=\"Valora la respuesta.\",\n",
        "            required=True,\n",
        "            labels=[\"👍\", \"👎\"]\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "kto_dataset = rg.Dataset(\n",
        "    name=\"kto-dataset\",\n",
        "    workspace=\"argilla\", # default name\n",
        "    client=client,\n",
        "    settings=kto_settings,\n",
        ")\n",
        "kto_dataset.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push los ejemplos que anotar"
      ],
      "metadata": {
        "id": "iTNlvAQR20j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "MODEL = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# Cargar el modelo y el tokenizador\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "\n",
        "# Crear una pipeline de generación de texto\n",
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Cargar el dataset de prompts\n",
        "prompts = load_dataset(\"somosnlp/prompt-translation-for-es\", split=\"train[:10]\")\n",
        "\n",
        "# Formatear según los campos esperados\n",
        "records = []\n",
        "for record in prompts:\n",
        "    prompt = record[\"generations\"][0]\n",
        "    outputs = gen_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    responses = [output[\"generated_text\"] for output in outputs]\n",
        "    record = {\"prompt\": prompt, \"response\": responses[0]}\n",
        "    records.append(record)\n",
        "\n",
        "kto_dataset.records.log(records=records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "9cd6e679614444bea3659983853c95cc",
            "8910dd93a493485e8c18bcb0b3fc81c8",
            "2bd77d413d1d4a15a17c0e04b4ddd294",
            "284c42f800b14fabbfec54167cb1ab95",
            "229166c9301e4c978a537d974ccee897",
            "0f5e25b985284ae9981eef5270db5d15",
            "3374824b4fd945b1bcb4e96473db70c2",
            "8ab6acce7f30430a954a4fbf5baa7556",
            "aca20fa542fe48a0a8dcf6092369afe9",
            "ccea4a6f7a584490b403d0a9dc44fd80",
            "5ba03906e88c49788c0e1f8be11c28aa"
          ]
        },
        "id": "lTb8KEuy05DK",
        "outputId": "f6a46fb5-e518-422d-c099-d377cc92f838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cd6e679614444bea3659983853c95cc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMfKzfiBaAYy"
      },
      "source": [
        "## Crear un dataset de preferencias: DPO\n",
        "\n",
        "Formato esperado:\n",
        "- `prompt`\n",
        "- `response-1`\n",
        "- `response-2`\n",
        "- `response_rating`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear el dataset en Argilla\n",
        "\n",
        "<!--\n",
        "dpo_settings = rg.Settings(\n",
        "    guidelines=\"Read the question...\",\n",
        "    fields = [\n",
        "        rg.TextField(name=\"prompt\", required=True),\n",
        "        rg.TextField(name=\"response-1\", required=True),\n",
        "        rg.TextField(name=\"response-2\", required=True),\n",
        "    ],\n",
        "    questions = [\n",
        "        rg.RatingQuestion(\n",
        "            name=\"response_ranking\",\n",
        "            title=\"Rank the responses\\n1: first response is better,\\n 2: second response is better\",\n",
        "            required=True,\n",
        "            values=[1, 2]\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "-->"
      ],
      "metadata": {
        "id": "Zu6tNXLT1wlv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spYSb4ZEaAYy",
        "outputId": "0925bd3b-b849-4645-c83a-278d03e50ad6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset(id=UUID('1479a93a-91a1-47d4-a059-62e6f9af4b83') inserted_at=datetime.datetime(2024, 8, 20, 16, 19, 6, 71382) updated_at=datetime.datetime(2024, 8, 20, 16, 19, 8, 328168) name='dpo-dataset' status='ready' guidelines='Read the question...' allow_extra_metadata=False distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('2f993734-0774-4016-a636-7c11251b2c64') last_activity_at=datetime.datetime(2024, 8, 20, 16, 19, 8, 328168))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import argilla as rg\n",
        "\n",
        "dpo_settings = rg.Settings(\n",
        "    guidelines=\"Lee la pregunta y...\",\n",
        "    fields = [\n",
        "        ...\n",
        "    ],\n",
        "    questions = [\n",
        "        ...\n",
        "    ],\n",
        ")\n",
        "\n",
        "dpo_dataset = rg.Dataset(\n",
        "    name=\"dpo-dataset\",\n",
        "    workspace=\"argilla\", # default name\n",
        "    client=client,\n",
        "    settings=dpo_settings,\n",
        ")\n",
        "dpo_dataset.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push los ejemplos que anotar"
      ],
      "metadata": {
        "id": "DddTmIzU11ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "GUZwcXuG0uhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset de preferencia más complejo: RLHF\n",
        "\n",
        "Formato esperado:\n",
        "- `prompt`\n",
        "- `response-1`\n",
        "- `response-2`\n",
        "- `response_rating`: 1, 2 o 3 en caso de empate\n",
        "- `correct_response`"
      ],
      "metadata": {
        "id": "pmptPDTEy5m8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear el dataset en Argilla\n",
        "\n",
        "<!--\n",
        "preference_settings = rg.Settings(\n",
        "    guidelines=\"Read the question...\",\n",
        "    fields = [\n",
        "        rg.TextField(name=\"prompt\", required=True),\n",
        "        rg.TextField(name=\"response-1\", required=True),\n",
        "        rg.TextField(name=\"response-2\", required=True)\n",
        "    ],\n",
        "    questions = [\n",
        "        rg.RatingQuestion(\n",
        "            name=\"response_ranking\",\n",
        "            title=\"Rank the responses\\n1: first response is better,\\n 2: second response is better,\\n3: both are equal\",\n",
        "            required=True,\n",
        "            values=[1, 2,3]\n",
        "        ),\n",
        "        rg.TextQuestion(\n",
        "            name=\"correct_response\",\n",
        "            title=\"If none of the responses are helpful and correct, provide the response\",\n",
        "            required=False\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "-->\n"
      ],
      "metadata": {
        "id": "brrCHzs_15wY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDeQO7ZCaAYx"
      },
      "outputs": [],
      "source": [
        "import argilla as rg\n",
        "\n",
        "preference_settings = rg.Settings(\n",
        "    guidelines=\"Lee la pregunta y...\",\n",
        "    fields = [\n",
        "        ...\n",
        "    ],\n",
        "    questions = [\n",
        "        ...\n",
        "    ],\n",
        ")\n",
        "\n",
        "preference_dataset = rg.Dataset(\n",
        "    name=\"preference-dataset\",\n",
        "    workspace=\"argilla\", # default name\n",
        "    client=client,\n",
        "    settings=preference_settings,\n",
        ")\n",
        "preference_dataset.create()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push los ejemplos que anotar"
      ],
      "metadata": {
        "id": "34DcJT-v19Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "LHa75Qu2zglW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push el dataset al Hub de Hugging Face"
      ],
      "metadata": {
        "id": "qCkXoF0X2BXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preference_dataset = client.datasets(name=\"preferences-dataset\")\n",
        "preference_dataset.to_hub(repo_id=\"<hf_handle>/<nombre_del_dataset>\")"
      ],
      "metadata": {
        "id": "uh91Pe_SzgGc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9cd6e679614444bea3659983853c95cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8910dd93a493485e8c18bcb0b3fc81c8",
              "IPY_MODEL_2bd77d413d1d4a15a17c0e04b4ddd294",
              "IPY_MODEL_284c42f800b14fabbfec54167cb1ab95"
            ],
            "layout": "IPY_MODEL_229166c9301e4c978a537d974ccee897"
          }
        },
        "8910dd93a493485e8c18bcb0b3fc81c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f5e25b985284ae9981eef5270db5d15",
            "placeholder": "​",
            "style": "IPY_MODEL_3374824b4fd945b1bcb4e96473db70c2",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "2bd77d413d1d4a15a17c0e04b4ddd294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ab6acce7f30430a954a4fbf5baa7556",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aca20fa542fe48a0a8dcf6092369afe9",
            "value": 0
          }
        },
        "284c42f800b14fabbfec54167cb1ab95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccea4a6f7a584490b403d0a9dc44fd80",
            "placeholder": "​",
            "style": "IPY_MODEL_5ba03906e88c49788c0e1f8be11c28aa",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "229166c9301e4c978a537d974ccee897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5e25b985284ae9981eef5270db5d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3374824b4fd945b1bcb4e96473db70c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ab6acce7f30430a954a4fbf5baa7556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca20fa542fe48a0a8dcf6092369afe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccea4a6f7a584490b403d0a9dc44fd80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba03906e88c49788c0e1f8be11c28aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}